{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfe4bfa",
   "metadata": {},
   "source": [
    "# Emission Rates Generation for ST Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64a955",
   "metadata": {},
   "source": [
    "## g/veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8433f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Buscando archivos de salida MOVES en: D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Input files\\Summary Reports para ER Generation\n",
      "   -> Cargando A: SummaryReportBody.tab\n",
      "   -> Cargando B: SummaryReportBody.tab\n",
      "   -> Cargando C: SummaryReportBody.tab\n",
      "üìÇ Cargando Poblaci√≥n desde: sourceTypePopulation.csv\n",
      "\n",
      "üîÑ Unificando archivos A, B y C...\n",
      "   -> Filas totales unificadas: 143,040\n",
      "üîÑ Agrupando emisiones por [Day, Hour, Source]...\n",
      "üîÑ Cruzando con tabla de Poblaci√≥n...\n",
      "üßÆ Calculando tasas (g/veh)...\n",
      "\n",
      "‚úÖ ¬°√âXITO! Lookup Table generada correctamente.\n",
      "   (Se han excluido las columnas: Rate_Fuel, Rate_Road, Rate_TotalEnergy)\n",
      "üìÇ Archivo guardado en: D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Output Files\\Generated emission rates\\Lookup_Table_Emission_Rates_g_per_veh.parquet\n",
      "\n",
      "Primeras 5 filas:\n",
      "   Day  Hour  Source          sourceTypeName  sourceTypePopulation  \\\n",
      "0    2     1      11              Motorcycle                   964   \n",
      "1    2     1      21           Passenger Car                  4960   \n",
      "2    2     1      22                    Taxi                    37   \n",
      "3    2     1      31         Passenger Truck                  2499   \n",
      "4    2     1      32  Light Commercial Truck                  2874   \n",
      "\n",
      "   Rate_CO2_per_veh  Rate_CO2_Equiv_per_veh  Rate_CO_per_veh  \\\n",
      "0        479.614108              482.859959        29.654564   \n",
      "1        200.954839              202.795766         9.842540   \n",
      "2        459.513514              462.594595        13.972973   \n",
      "3        305.397359              307.967587        10.560224   \n",
      "4        313.782533              317.620390        13.920320   \n",
      "\n",
      "   Rate_NOx_per_veh  Rate_Total_PM10_per_veh  Rate_Total_PM25_per_veh  \\\n",
      "0          0.897303                 0.033195                 0.026971   \n",
      "1          1.771976                 0.043750                 0.037702   \n",
      "2          4.189189                 0.000000                 0.000000   \n",
      "3          1.491397                 0.033613                 0.030812   \n",
      "4          1.875783                 0.060543                 0.055324   \n",
      "\n",
      "   Rate_TotalHC_per_veh  Rate_Distance_per_veh  \n",
      "0              1.656639               1.977178  \n",
      "1              0.294960               0.794758  \n",
      "2              0.270270               0.729730  \n",
      "3              0.537415               1.010804  \n",
      "4              0.895616               0.993737  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACI√ìN DE RUTAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Carpeta donde est√°n los archivos de SALIDA de MOVES (.tab o .csv) (A, B, C)\n",
    "MOVES_OUTPUT_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Input files\\Summary Reports para ER Generation\")\n",
    "\n",
    "# Carpeta donde est√° tu archivo 'sourceTypePopulation.csv'\n",
    "POPULATION_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Input files\\MOVES UIDB Population Table\")\n",
    "\n",
    "# --- NUEVA RUTA DE SALIDA PARA LAS TASAS (DI) ---\n",
    "OUTPUT_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Output Files\\Generated emission rates\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Mapeo de nombres para facilitar lectura\n",
    "SOURCE_TYPE_NAMES = {\n",
    "    11: \"Motorcycle\", 21: \"Passenger Car\", 22: \"Taxi\", 31: \"Passenger Truck\",\n",
    "    32: \"Light Commercial Truck\", 42: \"Transit Bus\", 43: \"School Bus\", 44: \"Other Buses\",\n",
    "    51: \"Refuse Truck\", 52: \"Single Unit Short-haul Truck\",\n",
    "    53: \"Single Unit Long-haul Truck\", 54: \"Motor Home\",\n",
    "    61: \"Combination Short-haul Truck\", 62: \"Combination Long-haul Truck\"\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FUNCIONES DE CARGA\n",
    "# ==============================================================================\n",
    "\n",
    "def identify_and_load_moves_files(root_folder):\n",
    "    \"\"\"\n",
    "    Busca archivos .tab o .csv, identifica si son A, B o C seg√∫n sus columnas\n",
    "    y los carga en un diccionario.\n",
    "    \"\"\"\n",
    "    path_root = Path(root_folder)\n",
    "    all_files = list(path_root.rglob(\"*.tab\")) + list(path_root.rglob(\"*.csv\"))\n",
    "    \n",
    "    dfs = {}\n",
    "    print(f\"üìÇ Buscando archivos de salida MOVES en: {root_folder}\")\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # Leer solo la primera fila para checar columnas\n",
    "            if file_path.suffix == '.tab':\n",
    "                sep = '\\t'\n",
    "            else:\n",
    "                sep = ','\n",
    "                \n",
    "            df_temp = pd.read_csv(file_path, sep=sep, nrows=1)\n",
    "            cols = df_temp.columns.tolist()\n",
    "            \n",
    "            # L√≥gica de identificaci√≥n\n",
    "            tag = None\n",
    "            if \"CO2_Equiv\" in cols:\n",
    "                tag = 'A'\n",
    "            elif \"NOx\" in cols:\n",
    "                tag = 'B'\n",
    "            elif \"TotalEnergy\" in cols:\n",
    "                tag = 'C'\n",
    "            \n",
    "            if tag:\n",
    "                print(f\"   -> Cargando {tag}: {file_path.name}\")\n",
    "                dfs[tag] = pd.read_csv(file_path, sep=sep)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {file_path.name}: {e}\")\n",
    "            \n",
    "    return dfs\n",
    "\n",
    "def load_population_table(pop_dir):\n",
    "    \"\"\"Carga la tabla de poblaci√≥n generada anteriormente.\"\"\"\n",
    "    pop_path = pop_dir / \"sourceTypePopulation.csv\"\n",
    "    if not pop_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå No se encuentra sourceTypePopulation.csv en {pop_dir}\")\n",
    "    \n",
    "    print(f\"üìÇ Cargando Poblaci√≥n desde: {pop_path.name}\")\n",
    "    df_pop = pd.read_csv(pop_path)\n",
    "    return df_pop[['sourceTypeID', 'sourceTypePopulation']]\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESAMIENTO Y C√ÅLCULO DE TASAS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_rates_lookup(dfs_moves, df_pop):\n",
    "    print(\"\\nüîÑ Unificando archivos A, B y C...\")\n",
    "    \n",
    "    # 1. Merge de A, B y C using standard MOVES keys\n",
    "    merge_keys = ['MOVESRunID', 'Year', 'Month', 'Day', 'Hour', 'Source', 'Fuel', 'ModelYr', 'Road']\n",
    "    \n",
    "    # Filtramos llaves que realmente existan en el DF A\n",
    "    keys_in_A = [k for k in merge_keys if k in dfs_moves['A'].columns]\n",
    "    \n",
    "    df_merged = dfs_moves['A']\n",
    "    if 'B' in dfs_moves:\n",
    "        df_merged = pd.merge(df_merged, dfs_moves['B'], on=keys_in_A, how='inner')\n",
    "    if 'C' in dfs_moves:\n",
    "        df_merged = pd.merge(df_merged, dfs_moves['C'], on=keys_in_A, how='inner')\n",
    "\n",
    "    print(f\"   -> Filas totales unificadas: {len(df_merged):,}\")\n",
    "\n",
    "    # 2. Agrupaci√≥n por Dia, Hora y Tipo Veh√≠culo (Source)\n",
    "    # Identificar columnas num√©ricas para sumar\n",
    "    numeric_cols = df_merged.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Llaves de agrupaci√≥n\n",
    "    group_keys = ['Day', 'Hour', 'Source']\n",
    "    \n",
    "    # Columnas a sumar (excluyendo llaves y metadatos)\n",
    "    cols_to_sum = [c for c in numeric_cols if c not in group_keys and c not in ['MOVESRunID', 'Year', 'Month', 'ModelYr']]\n",
    "    \n",
    "    print(\"üîÑ Agrupando emisiones por [Day, Hour, Source]...\")\n",
    "    df_grouped = df_merged.groupby(group_keys)[cols_to_sum].sum().reset_index()\n",
    "\n",
    "    # 3. Merge con Poblaci√≥n\n",
    "    print(\"üîÑ Cruzando con tabla de Poblaci√≥n...\")\n",
    "    df_final = pd.merge(df_grouped, df_pop, left_on='Source', right_on='sourceTypeID', how='left')\n",
    "    \n",
    "    # Validar match\n",
    "    missing_pop = df_final[df_final['sourceTypePopulation'].isna()]\n",
    "    if not missing_pop.empty:\n",
    "        print(f\"‚ö†Ô∏è ADVERTENCIA: {len(missing_pop)} filas no encontraron poblaci√≥n (SourceIDs: {missing_pop['Source'].unique()})\")\n",
    "        df_final['sourceTypePopulation'] = df_final['sourceTypePopulation'].fillna(1) \n",
    "\n",
    "    # 4. C√°lculo de Tasas (g/veh)\n",
    "    print(\"üßÆ Calculando tasas (g/veh)...\")\n",
    "    \n",
    "    rate_cols = []\n",
    "    for col in cols_to_sum:\n",
    "        rate_col_name = f\"Rate_{col}_per_veh\"\n",
    "        # Evitar divisi√≥n por cero\n",
    "        df_final[rate_col_name] = df_final[col] / df_final['sourceTypePopulation']\n",
    "        rate_cols.append(rate_col_name)\n",
    "\n",
    "    # 5. Limpieza Final y FILTRADO DE COLUMNAS NO DESEADAS\n",
    "    df_final['sourceTypeName'] = df_final['Source'].map(SOURCE_TYPE_NAMES).fillna(\"Unknown\")\n",
    "    \n",
    "    # Definir columnas a EXCLUIR expl√≠citamente\n",
    "    cols_to_exclude = ['Rate_Fuel_per_veh', 'Rate_Road_per_veh', 'Rate_TotalEnergy_per_veh']\n",
    "    \n",
    "    # Organizar columnas\n",
    "    final_cols = ['Day', 'Hour', 'Source', 'sourceTypeName', 'sourceTypePopulation'] + rate_cols\n",
    "    \n",
    "    # Filtro Final: Que existan en el DF Y que NO est√©n en la lista de exclusi√≥n\n",
    "    final_cols = [c for c in final_cols if c in df_final.columns and c not in cols_to_exclude]\n",
    "    \n",
    "    return df_final[final_cols]\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EJECUCI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Cargar Archivos MOVES\n",
    "    moves_data = identify_and_load_moves_files(MOVES_OUTPUT_DIR)\n",
    "    \n",
    "    if 'A' in moves_data: # M√≠nimo necesitamos el A\n",
    "        # 2. Cargar Poblaci√≥n\n",
    "        try:\n",
    "            df_population = load_population_table(POPULATION_DIR)\n",
    "            \n",
    "            # 3. Generar Lookup Table\n",
    "            df_rates = generate_rates_lookup(moves_data, df_population)\n",
    "            \n",
    "            # 4. Guardar como PARQUET\n",
    "            output_parquet = OUTPUT_DIR / \"Lookup_Table_Emission_Rates_g_per_veh.parquet\"\n",
    "            df_rates.to_parquet(output_parquet, index=False)\n",
    "            \n",
    "            print(f\"\\n‚úÖ ¬°√âXITO! Lookup Table generada correctamente.\")\n",
    "            print(f\"   (Se han excluido las columnas: Rate_Fuel, Rate_Road, Rate_TotalEnergy)\")\n",
    "            print(f\"üìÇ Archivo guardado en: {output_parquet}\")\n",
    "            print(\"\\nPrimeras 5 filas:\")\n",
    "            print(df_rates.head())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en el proceso: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No se encontr√≥ el archivo A (o faltan archivos) en la carpeta indicada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f33fbd",
   "metadata": {},
   "source": [
    "## g/km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c682a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FASE 1: CARGA DE DATOS ---\n",
      "Directorio base: D:\\MOVES\\Sensitivity Analysis RS14\\RS14\\Sensitivity Analysis Files\\All SBs\n",
      "Iniciando carga y unificaci√≥n de 16 Speed Bins...\n",
      "  [OK] Procesando SB01...\n",
      "  [OK] Procesando SB02...\n",
      "  [OK] Procesando SB03...\n",
      "  [OK] Procesando SB04...\n",
      "  [OK] Procesando SB05...\n",
      "  [OK] Procesando SB06...\n",
      "  [OK] Procesando SB07...\n",
      "  [OK] Procesando SB08...\n",
      "  [OK] Procesando SB09...\n",
      "  [OK] Procesando SB10...\n",
      "  [OK] Procesando SB11...\n",
      "  [OK] Procesando SB12...\n",
      "  [OK] Procesando SB13...\n",
      "  [OK] Procesando SB14...\n",
      "  [OK] Procesando SB15...\n",
      "  [OK] Procesando SB16...\n",
      "\n",
      ">> Carga completada. Filas crudas cargadas: 1,830,912\n",
      "\n",
      "--- FASE 2: PROCESAMIENTO Y C√ÅLCULO ---\n",
      "1. Normalizando tipos de datos num√©ricos...\n",
      "2. Eliminando Off-Network (Road ID 1)...\n",
      "3. Reestructurando datos (Melting)...\n",
      "4. Calculando tasas ponderadas (Suma Masa / Suma Distancia)...\n",
      "\n",
      "--- FASE 3: GENERACI√ìN DE FORMATO FINAL ---\n",
      "\n",
      "============================================================\n",
      "             RESUMEN DE PROCESO EXITOSO\n",
      "============================================================\n",
      "Tiempo Total de Ejecuci√≥n: 11.80 segundos\n",
      "------------------------------------------------------------\n",
      "1. Archivos Procesados:\n",
      "   - Bins encontrados: 16 (SB01 ... SB16)\n",
      "   - Filas crudas le√≠das: 1,830,912\n",
      "------------------------------------------------------------\n",
      "2. Datos Generados:\n",
      "   - Filas en archivo final: 14,432\n",
      "   - Columnas (Contaminantes): ['CO', 'CO2', 'CO2_Equiv', 'Energy', 'HC', 'NOx', 'PM10', 'PM25']\n",
      "------------------------------------------------------------\n",
      "3. Archivo Guardado:\n",
      "   - Ruta: D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Output Files\\Generated emission rates\\Lookup_Table_Emission_Rates_g_per_km.parquet\n",
      "   - Formato: Parquet (Optimizado)\n",
      "============================================================\n",
      "\n",
      "Vista previa (Primeras 5 filas):\n",
      "Pollutant  Day  Hour Road  Source  SpeedBin         CO         CO2  \\\n",
      "0            2     1    4      11         1  61.343358  815.325188   \n",
      "1            2     1    4      11         2  32.904762  468.699248   \n",
      "2            2     1    4      11         3  18.771303  298.611529   \n",
      "3            2     1    4      11         4  14.439223  253.399123   \n",
      "4            2     1    4      11         5  12.922932  224.755639   \n",
      "\n",
      "Pollutant   CO2_Equiv        Energy        HC       NOx      PM10      PM25  \n",
      "0          828.286341  1.134496e+07  5.585213  0.783835  0.029449  0.026942  \n",
      "1          475.303885  6.521778e+06  2.897243  0.495614  0.017544  0.016291  \n",
      "2          302.063283  4.155095e+06  1.573935  0.343985  0.011905  0.010652  \n",
      "3          255.869048  3.525958e+06  1.194236  0.283835  0.007519  0.006266  \n",
      "4          226.727444  3.127374e+06  0.994361  0.422932  0.016917  0.015038  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Iniciar cron√≥metro para medir rendimiento\n",
    "start_time = time.time()\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACI√ìN E INPUTS\n",
    "# ==============================================================================\n",
    "# Ruta principal\n",
    "BASE_INPUT_DIR = Path(r\"D:\\MOVES\\Sensitivity Analysis RS14\\RS14\\Sensitivity Analysis Files\\All SBs\")\n",
    "\n",
    "# Rutas de salida\n",
    "OUTPUT_PIVOTED_PARQUET = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Output Files\\Generated emission rates\\Lookup_Table_Emission_Rates_g_per_km.parquet\")\n",
    "\n",
    "# Claves para unir los archivos A, B y C\n",
    "MERGE_KEYS = ['Day', 'Hour', 'Source', 'Fuel', 'ModelYr', 'Road']\n",
    "\n",
    "# Mapa de contaminantes (Columnas MOVES -> Columnas Salida)\n",
    "pollutant_map = {\n",
    "    \"CO2_Equiv\":   \"CO2_Equiv\",\n",
    "    \"CO\":          \"CO\",\n",
    "    \"CO2\":         \"CO2\",\n",
    "    \"NOx\":         \"NOx\",          # Archivo B\n",
    "    \"Total_PM10\":  \"PM10\",         # Archivo B\n",
    "    \"Total_PM25\":  \"PM25\",         # Archivo B\n",
    "    \"TotalHC\":     \"HC\",           # Archivo C\n",
    "    \"TotalEnergy\": \"Energy\"        # Archivo C\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARGA Y FUSI√ìN DE ARCHIVOS (A + B + C)\n",
    "# ==============================================================================\n",
    "print(f\"--- FASE 1: CARGA DE DATOS ---\")\n",
    "print(f\"Directorio base: {BASE_INPUT_DIR}\")\n",
    "print(\"Iniciando carga y unificaci√≥n de 16 Speed Bins...\")\n",
    "\n",
    "all_dfs = []\n",
    "bins_encontrados = []\n",
    "\n",
    "for i in range(1, 17):\n",
    "    sb_name = f\"SB{i:02d}\"\n",
    "    sb_path = BASE_INPUT_DIR / sb_name\n",
    "    \n",
    "    # Rutas espec√≠ficas\n",
    "    path_a = sb_path / \"A\" / \"SummaryReportBody.tab\"\n",
    "    path_b = sb_path / \"B\" / \"SummaryReportBody.tab\"\n",
    "    path_c = sb_path / \"C\" / \"SummaryReportBody.tab\"\n",
    "    \n",
    "    # Verificar existencia\n",
    "    if not (path_a.exists() and path_b.exists() and path_c.exists()):\n",
    "        print(f\"  [X] {sb_name}: Faltan archivos, saltando.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  [OK] Procesando {sb_name}...\")\n",
    "    bins_encontrados.append(sb_name)\n",
    "\n",
    "    try:\n",
    "        # 1. Leer\n",
    "        df_a = pd.read_csv(path_a, sep='\\t', low_memory=False)\n",
    "        df_b = pd.read_csv(path_b, sep='\\t', low_memory=False)\n",
    "        df_c = pd.read_csv(path_c, sep='\\t', low_memory=False)\n",
    "\n",
    "        # 2. Fusionar (Merge)\n",
    "        df_merged = pd.merge(df_a, df_b, on=MERGE_KEYS, how='inner')\n",
    "        df_merged = pd.merge(df_merged, df_c, on=MERGE_KEYS, how='inner')\n",
    "\n",
    "        # 3. Etiquetar\n",
    "        df_merged['SpeedBin'] = i\n",
    "        \n",
    "        all_dfs.append(df_merged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Fallo en {sb_name}: {e}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    raise FileNotFoundError(\"ERROR CR√çTICO: No se cargaron datos. Verifica las rutas.\")\n",
    "\n",
    "# Concatenar todo en el DataFrame Maestro\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "raw_rows = len(df)\n",
    "print(f\"\\n>> Carga completada. Filas crudas cargadas: {raw_rows:,}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LIMPIEZA Y C√ÅLCULO DE TASAS\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- FASE 2: PROCESAMIENTO Y C√ÅLCULO ---\")\n",
    "\n",
    "# 1. Asegurar tipos num√©ricos\n",
    "numeric_cols = [\"Road\", \"Source\", \"Day\", \"Hour\", \"Distance\", \"SpeedBin\", \"ModelYr\"] + list(pollutant_map.keys())\n",
    "print(\"1. Normalizando tipos de datos num√©ricos...\")\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# 2. Filtrar Off-Network (Road != 1)\n",
    "print(\"2. Eliminando Off-Network (Road ID 1)...\")\n",
    "if \"Road\" in df.columns:\n",
    "    df_clean = df[df[\"Road\"] != 1].copy()\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "# 3. Convertir a Formato Largo (Melt)\n",
    "print(\"3. Reestructurando datos (Melting)...\")\n",
    "long_rows = []\n",
    "present_pollutants = [c for c in pollutant_map.keys() if c in df_clean.columns]\n",
    "\n",
    "for col in present_pollutants:\n",
    "    # Columnas necesarias\n",
    "    cols_to_keep = [\"Day\", \"Hour\", \"Road\", \"Source\", \"SpeedBin\", \"Distance\", col]\n",
    "    \n",
    "    if all(c in df_clean.columns for c in cols_to_keep):\n",
    "        sub = df_clean[cols_to_keep].copy()\n",
    "        sub = sub.rename(columns={col: \"raw_value\"})\n",
    "        sub[\"Pollutant\"] = pollutant_map[col] # Renombrar a nombre limpio\n",
    "        sub[\"mass_g\"] = sub[\"raw_value\"]\n",
    "        \n",
    "        # Optimizaci√≥n memoria\n",
    "        sub[\"Pollutant\"] = sub[\"Pollutant\"].astype('category')\n",
    "        \n",
    "        long_rows.append(sub[[\"Day\", \"Hour\", \"Road\", \"Source\", \"SpeedBin\", \"Pollutant\", \"mass_g\", \"Distance\"]])\n",
    "\n",
    "long_df = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "# 4. Limpieza de Nulos y Ceros\n",
    "long_df[\"Distance\"] = long_df[\"Distance\"].fillna(0)\n",
    "long_df[\"mass_g\"] = long_df[\"mass_g\"].fillna(0)\n",
    "long_df = long_df.query(\"mass_g > 0 or Distance > 0\") # Eliminar filas vac√≠as\n",
    "\n",
    "# 5. Agregaci√≥n y C√°lculo de Tasas (g/km)\n",
    "print(\"4. Calculando tasas ponderadas (Suma Masa / Suma Distancia)...\")\n",
    "grp = long_df.groupby([\"Day\", \"Hour\", \"Road\", \"Source\", \"SpeedBin\", \"Pollutant\"], \n",
    "                      observed=True, as_index=False).agg(\n",
    "    total_mass_g=(\"mass_g\", \"sum\"),\n",
    "    total_distance_km=(\"Distance\", \"sum\")\n",
    ")\n",
    "\n",
    "# F√≥rmula: g/km (evitando divisi√≥n por cero)\n",
    "grp[\"rate_per_km\"] = np.where(grp[\"total_distance_km\"] > 0,\n",
    "                              grp[\"total_mass_g\"] / grp[\"total_distance_km\"],\n",
    "                              0.0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PIVOTE Y GUARDADO FINAL\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- FASE 3: GENERACI√ìN DE FORMATO FINAL ---\")\n",
    "\n",
    "# Pivotar para tener contaminantes como columnas\n",
    "df_pivot = grp.pivot_table(\n",
    "    index=['Day', 'Hour', 'Road', 'Source', 'SpeedBin'],\n",
    "    columns='Pollutant', \n",
    "    values='rate_per_km', \n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "df_pivot.reset_index(inplace=True)\n",
    "df_pivot['Road'] = df_pivot['Road'].astype(str) # Road como texto para consistencia\n",
    "df_pivot = df_pivot.fillna(0)\n",
    "\n",
    "# Guardar\n",
    "Path(BASE_INPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df_pivot.to_parquet(OUTPUT_PIVOTED_PARQUET, index=False)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. RESUMEN DE SALIDA (OUTPUT REPORT)\n",
    "# ==============================================================================\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"             RESUMEN DE PROCESO EXITOSO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Tiempo Total de Ejecuci√≥n: {duration:.2f} segundos\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"1. Archivos Procesados:\")\n",
    "print(f\"   - Bins encontrados: {len(bins_encontrados)} ({bins_encontrados[0]} ... {bins_encontrados[-1]})\")\n",
    "print(f\"   - Filas crudas le√≠das: {raw_rows:,}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"2. Datos Generados:\")\n",
    "print(f\"   - Filas en archivo final: {len(df_pivot):,}\")\n",
    "print(f\"   - Columnas (Contaminantes): {list(df_pivot.columns[5:])}\") # Omitimos las 5 llaves\n",
    "print(\"-\" * 60)\n",
    "print(f\"3. Archivo Guardado:\")\n",
    "print(f\"   - Ruta: {OUTPUT_PIVOTED_PARQUET}\")\n",
    "print(f\"   - Formato: Parquet (Optimizado)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preview final\n",
    "print(\"\\nVista previa (Primeras 5 filas):\")\n",
    "print(df_pivot.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
