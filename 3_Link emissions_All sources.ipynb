{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae1bbc",
   "metadata": {},
   "source": [
    "## 1. Procesamiento de Auto y Transporte de Carga (DD + DI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4a37e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš— INICIANDO PROCESAMIENTO: AUTO Y CARGA...\n",
      "\n",
      "--- A. Calculando Emisiones Dependientes de la Distancia (DD) ---\n",
      "   âœ… Emisiones DD y GeometrÃ­a de Auto guardadas.\n",
      "\n",
      "--- B. Calculando Emisiones Independientes de la Distancia (DI) ---\n",
      "   âœ… Emisiones DI de Auto guardadas.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACIÃ“N - AUTO Y CARGA\n",
    "# ==============================================================================\n",
    "print(\"ðŸš— INICIANDO PROCESAMIENTO: AUTO Y CARGA...\")\n",
    "\n",
    "VISUM_BASE_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\NB3_OutputData\\VISUM Files V2\\Link files with County\\20251030\\Auto\\Links\")\n",
    "INTEGRATION_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\")\n",
    "INTEGRATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUTA_FACTORES_DD = INTEGRATION_DIR / \"Output Files\" / \"Generated emission rates\" / \"Lookup_Table_Emission_Rates_g_per_km.parquet\"\n",
    "RUTA_FACTORES_DI = INTEGRATION_DIR / \"Output Files\" / \"Generated emission rates\" / \"Lookup_Table_Emission_Rates_g_per_veh.parquet\"\n",
    "\n",
    "FILES_MAPPING = {\n",
    "    \"EM\": \"scenario=AutoEM\", \"AP\": \"scenario=AutoAP\", \"LM\": \"scenario=AutoLM\",\n",
    "    \"MP\": \"scenario=AutoMP\", \"EA\": \"scenario=AutoEA\", \"PP\": \"scenario=AutoPP\", \"EV\": \"scenario=AutoEV\"\n",
    "}\n",
    "\n",
    "HOURS_MAPPING = {\n",
    "    \"EM\": [22, 23, 0, 1, 2, 3, 4, 5], \"AP\": [6, 7, 8, 9], \"LM\": [10, 11],\n",
    "    \"MP\": [12], \"EA\": [13, 14, 15, 16], \"PP\": [17, 18], \"EV\": [19, 20, 21]\n",
    "}\n",
    "\n",
    "VOLUME_FACTORS = {\n",
    "    \"EM\": 1.0, \"AP\": 0.290781, \"LM\": 0.503228, \"MP\": 1.0, \n",
    "    \"EA\": 0.250912, \"PP\": 0.512564, \"EV\": 0.406226 \n",
    "}\n",
    "\n",
    "LDVS_HDVS_A_MAPPING = {\n",
    "    11: 0.146429251, 21: 0.320888794, 22: 0.104594992,\n",
    "    31: 0.190505261, 32: 0.220278756, 43: 0.002389634, 44: 0.014913313\n",
    "}\n",
    "\n",
    "POLLUTANTS = ['CO2', 'CO2_Equiv', 'CO', 'NOx', 'Total_PM25', 'Total_PM10', 'TotalHC', 'CH4', 'N2O']\n",
    "\n",
    "def clean_visum_string(val):\n",
    "    if pd.isna(val) or val == \"\": return 0.0\n",
    "    if isinstance(val, (int, float)): return float(val)\n",
    "    return float(re.sub(r'[^\\d\\.]', '', str(val).lower()))\n",
    "\n",
    "def get_moves_speed_bin(speed_kmh):\n",
    "    speed_mph = speed_kmh / 1.60934\n",
    "    bins = [0, 2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5, 52.5, 57.5, 62.5, 67.5, 72.5]\n",
    "    for i, limite in enumerate(bins[1:], start=1):\n",
    "        if speed_mph < limite: return i\n",
    "    return 16\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PROCESAMIENTO DD (DISTANCE DEPENDENT)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- A. Calculando Emisiones Dependientes de la Distancia (DD) ---\")\n",
    "df_ef_dd = pd.read_parquet(RUTA_FACTORES_DD)\n",
    "\n",
    "# ðŸŒŸ MAGIA DE NOMBRES: Renombramos las columnas para que empaten perfectamente\n",
    "df_ef_dd = df_ef_dd.rename(columns={'PM10': 'Total_PM10', 'PM25': 'Total_PM25', 'HC': 'TotalHC'})\n",
    "\n",
    "cols_presentes = [c for c in POLLUTANTS if c in df_ef_dd.columns]\n",
    "ef_lookup_dd = df_ef_dd.groupby(['Source', 'SpeedBin'])[cols_presentes].mean()\n",
    "\n",
    "all_hourly_emissions_dd = []\n",
    "geometry_records = []\n",
    "procesed_links = set()\n",
    "all_visum_dfs = [] \n",
    "\n",
    "for periodo, folder_name in FILES_MAPPING.items():\n",
    "    found_files = list((VISUM_BASE_DIR / folder_name).glob(\"Links.parquet\"))\n",
    "    if not found_files: continue\n",
    "        \n",
    "    df_links = pd.read_parquet(found_files[0])\n",
    "    if 'NO' in df_links.columns: df_links.rename(columns={'NO': 'LinkID'}, inplace=True)\n",
    "    elif 'no' in df_links.columns: df_links.rename(columns={'no': 'LinkID'}, inplace=True)\n",
    "    \n",
    "    df_links['L_km'] = df_links['LENGTH'].apply(clean_visum_string)\n",
    "    col_spd = next((c for c in df_links.columns if 'VCUR' in c.upper() and 'AUTO_C' in c.upper()), None)\n",
    "    df_links['V_kmh'] = df_links[col_spd].apply(clean_visum_string) if col_spd else 0.0\n",
    "    df_links['SpeedBin'] = df_links['V_kmh'].apply(get_moves_speed_bin)\n",
    "    \n",
    "    col_vol = next((c for c in df_links.columns if 'VOLVEH' in c.upper() and 'AUTO_C' in c.upper()), None)\n",
    "    df_links['Q_periodo'] = df_links[col_vol].apply(clean_visum_string) if col_vol else 0.0\n",
    "    \n",
    "    factor_vol = VOLUME_FACTORS.get(periodo, 1.0)\n",
    "    df_links['VOL_AUTO_C'] = (df_links['Q_periodo'] / factor_vol) if factor_vol not in [0, 1.0] else df_links['Q_periodo']\n",
    "    df_links['VOL_CA1_C'] = df_links[next((c for c in df_links.columns if 'VOL' in c.upper() and 'CA1_C' in c.upper()), 'Q_periodo')].apply(clean_visum_string) / factor_vol\n",
    "    df_links['VOL_CA2_C'] = df_links[next((c for c in df_links.columns if 'VOL' in c.upper() and 'CA2_C' in c.upper()), 'Q_periodo')].apply(clean_visum_string) / factor_vol\n",
    "    df_links['VOL_CA_BD'] = df_links[next((c for c in df_links.columns if 'VOL' in c.upper() and 'CA_BD' in c.upper()), 'Q_periodo')].apply(clean_visum_string) / factor_vol\n",
    "    df_links['VOL_CU_C'] = df_links[next((c for c in df_links.columns if 'VOL' in c.upper() and 'CU_C' in c.upper()), 'Q_periodo')].apply(clean_visum_string) / factor_vol\n",
    "    df_links['Periodo'] = periodo\n",
    "    all_visum_dfs.append(df_links[['LinkID', 'Periodo', 'VOL_AUTO_C', 'VOL_CA1_C', 'VOL_CA2_C', 'VOL_CA_BD', 'VOL_CU_C']].copy())\n",
    "\n",
    "    if 'WKTPOLYWGS84' in df_links.columns:\n",
    "        nuevos_links = df_links[~df_links['LinkID'].isin(procesed_links)]\n",
    "        if not nuevos_links.empty:\n",
    "            geometry_records.append(nuevos_links[['LinkID', 'WKTPOLYWGS84']].copy())\n",
    "            procesed_links.update(nuevos_links['LinkID'].tolist())\n",
    "\n",
    "    horas_periodo = HOURS_MAPPING.get(periodo, [])\n",
    "    if len(horas_periodo) == 0: continue\n",
    "\n",
    "    df_activos = df_links[df_links['Q_periodo'] > 0].copy()\n",
    "    for source_id, fraction in LDVS_HDVS_A_MAPPING.items():\n",
    "        vol_hora_source = (df_activos['Q_periodo'] * fraction) / len(horas_periodo)\n",
    "        try: factor_df = ef_lookup_dd.loc[source_id]\n",
    "        except KeyError: continue \n",
    "            \n",
    "        temp_emissions = pd.DataFrame({'LinkID': df_activos['LinkID']})\n",
    "        for pol in cols_presentes:\n",
    "            temp_emissions[pol] = vol_hora_source * df_activos['L_km'] * df_activos['SpeedBin'].map(factor_df[pol]).fillna(0)\n",
    "            \n",
    "        for h in horas_periodo:\n",
    "            df_h = temp_emissions.copy()\n",
    "            df_h['Hour'] = h + 1 \n",
    "            df_h['Source'] = source_id\n",
    "            df_h['Day'] = 5 \n",
    "            all_hourly_emissions_dd.append(df_h)\n",
    "\n",
    "df_final_dd = pd.concat(all_hourly_emissions_dd, ignore_index=True)\n",
    "df_final_dd.to_parquet(INTEGRATION_DIR / \"Link_Emissions_DistBased_Day_5.parquet\", index=False)\n",
    "pd.concat(geometry_records, ignore_index=True).to_parquet(INTEGRATION_DIR / \"Links_Geometry_Master.parquet\", index=False)\n",
    "print(\"   âœ… Emisiones DD y GeometrÃ­a de Auto guardadas.\")\n",
    "del all_hourly_emissions_dd, df_final_dd; gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESAMIENTO DI OPTIMIZADO (VECTORIZADO)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- B. Calculando Emisiones Independientes de la Distancia (DI) ---\")\n",
    "df_visum_master = pd.concat(all_visum_dfs, ignore_index=True)\n",
    "del all_visum_dfs; gc.collect()\n",
    "\n",
    "df_long = df_visum_master.melt(id_vars=['LinkID', 'Periodo'],\n",
    "                               value_vars=['VOL_AUTO_C', 'VOL_CA1_C', 'VOL_CA2_C', 'VOL_CA_BD', 'VOL_CU_C'],\n",
    "                               var_name='TSYS', value_name='Volume')\n",
    "df_long = df_long[df_long['Volume'] > 0]\n",
    "del df_visum_master; gc.collect()\n",
    "\n",
    "map_rows = []\n",
    "total_auto = sum(LDVS_HDVS_A_MAPPING.values())\n",
    "for k, v in LDVS_HDVS_A_MAPPING.items():\n",
    "    map_rows.append({'TSYS': 'VOL_AUTO_C', 'Source': k, 'Share': v / total_auto})\n",
    "map_rows.extend([\n",
    "    {'TSYS': 'VOL_CA1_C', 'Source': 61, 'Share': 1.0},\n",
    "    {'TSYS': 'VOL_CA2_C', 'Source': 61, 'Share': 1.0},\n",
    "    {'TSYS': 'VOL_CA_BD', 'Source': 61, 'Share': 1.0},\n",
    "    {'TSYS': 'VOL_CU_C', 'Source': 52, 'Share': 1.0}\n",
    "])\n",
    "df_map = pd.DataFrame(map_rows)\n",
    "\n",
    "df_long = df_long.merge(df_map, on='TSYS', how='inner')\n",
    "df_long['Source_Volume'] = df_long['Volume'] * df_long['Share']\n",
    "df_long = df_long.groupby(['LinkID', 'Periodo', 'Source'], as_index=False)['Source_Volume'].sum()\n",
    "\n",
    "df_totals = df_long.groupby(['Periodo', 'Source'], as_index=False)['Source_Volume'].sum()\n",
    "df_totals.rename(columns={'Source_Volume': 'Total_Volume'}, inplace=True)\n",
    "df_long = df_long.merge(df_totals, on=['Periodo', 'Source'])\n",
    "df_long['Network_Fraction'] = (df_long['Source_Volume'] / df_long['Total_Volume']).astype('float32')\n",
    "df_long.drop(columns=['Source_Volume', 'Total_Volume'], inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "df_rates = pd.read_parquet(RUTA_FACTORES_DI)\n",
    "df_day = df_rates[df_rates['Day'] == 5].copy()\n",
    "rate_cols = [c for c in df_rates.columns if c.startswith('Rate_')]\n",
    "macro_cols = ['Hour', 'Source']\n",
    "for col in rate_cols:\n",
    "    base = col.replace('Rate_', '').replace('_per_veh', '')\n",
    "    df_day[base] = df_day[col] * df_day['sourceTypePopulation']\n",
    "    macro_cols.append(base)\n",
    "emission_cols = [c for c in macro_cols if c not in ['Hour', 'Source']]\n",
    "\n",
    "final_di_chunks = []\n",
    "for periodo, horas in HOURS_MAPPING.items():\n",
    "    if not horas: continue\n",
    "    df_periodo = df_long[df_long['Periodo'] == periodo]\n",
    "    if df_periodo.empty: continue\n",
    "\n",
    "    for h in horas:\n",
    "        hour_val = h + 1\n",
    "        df_macro_h = df_day[df_day['Hour'] == hour_val]\n",
    "        if df_macro_h.empty: continue\n",
    "\n",
    "        df_merged = pd.merge(df_periodo, df_macro_h[macro_cols], on='Source', how='inner')\n",
    "        for col in emission_cols:\n",
    "            df_merged[col] = df_merged[col] * df_merged['Network_Fraction']\n",
    "\n",
    "        df_merged['Hour'] = hour_val\n",
    "        df_merged['Day'] = 5\n",
    "        final_di_chunks.append(df_merged[['LinkID', 'Day', 'Hour', 'Source'] + emission_cols])\n",
    "\n",
    "df_final_di = pd.concat(final_di_chunks, ignore_index=True)\n",
    "df_final_di.to_parquet(INTEGRATION_DIR / \"Link_Emissions_VolBased_Day_5.parquet\", index=False)\n",
    "print(\"   âœ… Emisiones DI de Auto guardadas.\\n\")\n",
    "del df_long, final_di_chunks, df_final_di; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d2aa2",
   "metadata": {},
   "source": [
    "## 2. Procesamiento de Transporte PÃºblico (DD + DI Continuo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1795d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸšŒ INICIANDO PROCESAMIENTO: TRANSPORTE PÃšBLICO...\n",
      "\n",
      "--- A. Calculando Emisiones Dependientes de la Distancia (DD) TP ---\n",
      "   âœ… Emisiones DD y GeometrÃ­a de TP guardadas.\n",
      "\n",
      "--- B. Calculando Emisiones Independientes de la Distancia (DI) TP ---\n",
      "   âœ… Emisiones DI de TP guardadas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACIÃ“N - TRANSPORTE PÃšBLICO\n",
    "# ==============================================================================\n",
    "print(\"ðŸšŒ INICIANDO PROCESAMIENTO: TRANSPORTE PÃšBLICO...\")\n",
    "\n",
    "DIR_PROFILES = Path(r\"D:\\VISUM\\Archivos de integraciÃ³n MOVES_VISUM\\InputData\\20251002\\Transit\\Timeprofiles\")\n",
    "DIR_TP_ITEMS = Path(r\"D:\\VISUM\\Archivos de integraciÃ³n MOVES_VISUM\\InputData\\20251002\\Transit\\Timeprofileitems\")\n",
    "DIR_LR_ITEMS = Path(r\"D:\\VISUM\\Archivos de integraciÃ³n MOVES_VISUM\\InputData\\20251002\\Transit\\Linerouteitems\")\n",
    "\n",
    "INTEGRATION_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\")\n",
    "RUTA_FACTORES_DD = INTEGRATION_DIR / \"Output Files\" / \"Generated emission rates\" / \"Lookup_Table_Emission_Rates_g_per_km.parquet\"\n",
    "RUTA_FACTORES_DI = INTEGRATION_DIR / \"Output Files\" / \"Generated emission rates\" / \"Lookup_Table_Emission_Rates_g_per_veh.parquet\"\n",
    "\n",
    "TRANSIT_BUS_ID = 42\n",
    "HEADWAY_IS_SECONDS = True \n",
    "POLLUTANTS = ['CO2', 'CO2_Equiv', 'CO', 'NOx', 'Total_PM25', 'Total_PM10', 'TotalHC', 'CH4', 'N2O']\n",
    "\n",
    "FOLDER_MAPPING = {\n",
    "    \"EM\": \"scenario=TransitEM\", \"AP\": \"scenario=TransitAP\", \"LM\": \"scenario=TransitLM\",\n",
    "    \"MP\": \"scenario=TransitMP\", \"EA\": \"scenario=TransitEA\", \"PP\": \"scenario=TransitPP\", \"EV\": \"scenario=TransitEV\"\n",
    "}\n",
    "HOURS_MAPPING = {\n",
    "    \"EM\": [22, 23, 0, 1, 2, 3, 4, 5], \"AP\": [6, 7, 8, 9], \"LM\": [10, 11],\n",
    "    \"MP\": [12], \"EA\": [13, 14, 15, 16], \"PP\": [17, 18], \"EV\": [19, 20, 21]\n",
    "}\n",
    "\n",
    "def clean_visum_string(val):\n",
    "    if pd.isna(val) or val == \"\": return 0.0\n",
    "    if isinstance(val, (int, float)): return float(val)\n",
    "    return float(re.sub(r'[^\\d\\.]', '', str(val).lower()))\n",
    "\n",
    "def get_moves_speed_bin(speed_kmh):\n",
    "    speed_mph = speed_kmh / 1.60934\n",
    "    bins = [0, 2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5, 52.5, 57.5, 62.5, 67.5, 72.5]\n",
    "    for i, limite in enumerate(bins[1:], start=1):\n",
    "        if speed_mph < limite: return i\n",
    "    return 16\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PROCESAMIENTO DD (RUTAS CONTINUAS)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- A. Calculando Emisiones Dependientes de la Distancia (DD) TP ---\")\n",
    "df_ef = pd.read_parquet(RUTA_FACTORES_DD)\n",
    "\n",
    "# ðŸŒŸ MAGIA DE NOMBRES PARA TP TAMBIÃ‰N\n",
    "df_ef = df_ef.rename(columns={'PM10': 'Total_PM10', 'PM25': 'Total_PM25', 'HC': 'TotalHC'})\n",
    "\n",
    "cols_presentes = [c for c in POLLUTANTS if c in df_ef.columns]\n",
    "ef_lookup = df_ef.groupby(['Source', 'SpeedBin'])[cols_presentes].mean()\n",
    "\n",
    "all_hourly_emissions = []\n",
    "wkt_to_id = {} \n",
    "current_link_id = 9000000 \n",
    "geometry_records = []\n",
    "\n",
    "for periodo, folder in FOLDER_MAPPING.items():\n",
    "    file_prof = list((DIR_PROFILES / folder).glob(\"*.parquet\"))\n",
    "    file_tp = list((DIR_TP_ITEMS / folder).glob(\"*.parquet\"))\n",
    "    file_lr = list((DIR_LR_ITEMS / folder).glob(\"*.parquet\"))\n",
    "    \n",
    "    if not (file_prof and file_tp and file_lr): continue\n",
    "        \n",
    "    df_prof = pd.read_parquet(file_prof[0])\n",
    "    df_tp = pd.read_parquet(file_tp[0])\n",
    "    df_lr = pd.read_parquet(file_lr[0])\n",
    "    \n",
    "    df_prof.columns = [c.strip().upper() for c in df_prof.columns]\n",
    "    df_tp.columns = [c.strip().upper() for c in df_tp.columns]\n",
    "    df_lr.columns = [c.strip().upper() for c in df_lr.columns]\n",
    "    \n",
    "    merge_keys = ['LINENAME', 'LINEROUTENAME', 'DIRECTIONCODE']\n",
    "    df_merged = pd.merge(df_lr, df_prof, on=merge_keys, how='inner')\n",
    "    df_merged = pd.merge(df_merged, df_tp[merge_keys + ['LRITEMINDEX', 'VEL']], \n",
    "                         left_on=merge_keys + ['INDEX'], right_on=merge_keys + ['LRITEMINDEX'], how='left')\n",
    "    \n",
    "    df_merged.sort_values(merge_keys + ['INDEX'], inplace=True)\n",
    "    df_merged['VEL'] = df_merged.groupby(merge_keys)['VEL'].ffill().bfill()\n",
    "    df_merged = df_merged[df_merged['WKTPOLYWGS84'].notna() & (df_merged['WKTPOLYWGS84'] != 'LINESTRING EMPTY')]\n",
    "    \n",
    "    horas = HOURS_MAPPING.get(periodo, [])\n",
    "    hw_col = f\"HEADWAY{periodo}\"\n",
    "    if len(horas) == 0 or hw_col not in df_merged.columns: continue\n",
    "        \n",
    "    df_merged['HEADWAY_SEC'] = df_merged[hw_col].apply(clean_visum_string) * (1.0 if HEADWAY_IS_SECONDS else 60.0)\n",
    "    df_merged['Vol_Hora'] = np.where(df_merged['HEADWAY_SEC'] > 0, 3600 / df_merged['HEADWAY_SEC'], 0)\n",
    "    df_merged['L_km'] = df_merged['POSTLENGTH'].apply(clean_visum_string)\n",
    "    df_merged['V_kmh'] = df_merged['VEL'].apply(clean_visum_string)\n",
    "    df_merged['SpeedBin'] = df_merged['V_kmh'].apply(get_moves_speed_bin)\n",
    "    \n",
    "    df_activos = df_merged[df_merged['Vol_Hora'] > 0].copy()\n",
    "    if df_activos.empty: continue\n",
    "        \n",
    "    for idx, row in df_activos.iterrows():\n",
    "        wkt = row['WKTPOLYWGS84']\n",
    "        if wkt not in wkt_to_id:\n",
    "            wkt_to_id[wkt] = current_link_id\n",
    "            geometry_records.append({'LinkID': current_link_id, 'WKTPOLYWGS84': wkt})\n",
    "            current_link_id += 1\n",
    "        df_activos.at[idx, 'LinkID'] = wkt_to_id[wkt]\n",
    "\n",
    "    try: factor_df = ef_lookup.loc[TRANSIT_BUS_ID]\n",
    "    except KeyError: continue \n",
    "        \n",
    "    temp_emissions = pd.DataFrame({'LinkID': df_activos['LinkID']})\n",
    "    for pol in cols_presentes:\n",
    "        temp_emissions[pol] = df_activos['Vol_Hora'] * df_activos['L_km'] * df_activos['SpeedBin'].map(factor_df[pol]).fillna(0)\n",
    "        \n",
    "    for h in horas:\n",
    "        df_h = temp_emissions.copy()\n",
    "        df_h['Hour'] = h + 1; df_h['Source'] = TRANSIT_BUS_ID; df_h['Day'] = 5 \n",
    "        all_hourly_emissions.append(df_h)\n",
    "\n",
    "df_final_dd = pd.concat(all_hourly_emissions, ignore_index=True)\n",
    "df_final_dd = df_final_dd.groupby(['LinkID', 'Day', 'Hour', 'Source'], as_index=False)[cols_presentes].sum()\n",
    "df_final_dd.to_parquet(INTEGRATION_DIR / \"Transit_Emissions_DistBased_Day_5.parquet\", index=False)\n",
    "pd.DataFrame(geometry_records).to_parquet(INTEGRATION_DIR / \"Transit_Geometry_Master.parquet\", index=False)\n",
    "print(\"   âœ… Emisiones DD y GeometrÃ­a de TP guardadas.\")\n",
    "\n",
    "del all_hourly_emissions, df_final_dd, geometry_records; gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESAMIENTO DI (DISTANCE INDEPENDENT)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- B. Calculando Emisiones Independientes de la Distancia (DI) TP ---\")\n",
    "network_data = []\n",
    "\n",
    "for periodo, folder in FOLDER_MAPPING.items():\n",
    "    file_prof = list((DIR_PROFILES / folder).glob(\"*.parquet\"))\n",
    "    file_lr = list((DIR_LR_ITEMS / folder).glob(\"*.parquet\"))\n",
    "    if not (file_prof and file_lr): continue\n",
    "        \n",
    "    df_prof, df_lr = pd.read_parquet(file_prof[0]), pd.read_parquet(file_lr[0])\n",
    "    df_prof.columns, df_lr.columns = [c.strip().upper() for c in df_prof.columns], [c.strip().upper() for c in df_lr.columns]\n",
    "    \n",
    "    df_merged = pd.merge(df_lr, df_prof, on=['LINENAME', 'LINEROUTENAME', 'DIRECTIONCODE'], how='inner')\n",
    "    df_merged = df_merged[df_merged['WKTPOLYWGS84'].notna() & (df_merged['WKTPOLYWGS84'] != 'LINESTRING EMPTY')]\n",
    "    \n",
    "    horas, hw_col = HOURS_MAPPING.get(periodo, []), f\"HEADWAY{periodo}\"\n",
    "    if len(horas) == 0 or hw_col not in df_merged.columns: continue\n",
    "        \n",
    "    df_merged['HEADWAY_SEC'] = df_merged[hw_col].apply(clean_visum_string) * (1.0 if HEADWAY_IS_SECONDS else 60.0)\n",
    "    df_merged['Vol_Hora'] = np.where(df_merged['HEADWAY_SEC'] > 0, 3600 / df_merged['HEADWAY_SEC'], 0)\n",
    "    \n",
    "    for _, row in df_merged[df_merged['Vol_Hora'] > 0].iterrows():\n",
    "        link_id = wkt_to_id.get(row['WKTPOLYWGS84']) \n",
    "        if link_id:\n",
    "            for h in horas: network_data.append((link_id, h + 1, TRANSIT_BUS_ID, row['Vol_Hora']))\n",
    "\n",
    "df_net = pd.DataFrame(network_data, columns=['LinkID', 'Hour', 'Source', 'Link_Volume'])\n",
    "df_net = df_net.groupby(['LinkID', 'Hour', 'Source'], as_index=False)['Link_Volume'].sum() \n",
    "df_net['Network_Fraction'] = df_net['Link_Volume'] / df_net.groupby(['Hour', 'Source'])['Link_Volume'].transform('sum')\n",
    "\n",
    "df_rates = pd.read_parquet(RUTA_FACTORES_DI)\n",
    "df_day = df_rates[df_rates['Day'] == 5].copy()\n",
    "rate_cols = [c for c in df_rates.columns if c.startswith('Rate_')]\n",
    "\n",
    "macro_cols = ['Hour', 'Source']\n",
    "for col in rate_cols:\n",
    "    base = col.replace('Rate_', '').replace('_per_veh', '')\n",
    "    df_day[base] = df_day[col] * df_day['sourceTypePopulation']\n",
    "    macro_cols.append(base)\n",
    "    \n",
    "df_final_di = pd.merge(df_net, df_day[macro_cols], on=['Hour', 'Source'], how='inner')\n",
    "emission_cols = [c for c in macro_cols if c not in ['Hour', 'Source']]\n",
    "\n",
    "for col in emission_cols: df_final_di[col] = df_final_di[col] * df_final_di['Network_Fraction']\n",
    "df_final_di['Day'] = 5\n",
    "df_final_di[['LinkID', 'Day', 'Hour', 'Source'] + emission_cols].to_parquet(INTEGRATION_DIR / \"Transit_Emissions_VolBased_Day_5.parquet\", index=False)\n",
    "print(\"   âœ… Emisiones DI de TP guardadas.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c6709",
   "metadata": {},
   "source": [
    "## 3. UnificaciÃ³n Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2487138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ INICIANDO LA GRAN UNIFICACIÃ“N (Auto + Carga + TP)...\n",
      "   -> Integrando Auto y Carga...\n",
      "   -> Integrando Transporte PÃºblico...\n",
      "   -> Fusionando Flotas sobre la misma geometrÃ­a fÃ­sica...\n",
      "   -> Transformando a GeoDataFrame...\n",
      "   -> Recalculando longitud fÃ­sica exacta (UTM Monterrey)...\n",
      "   -> Calculando densidad (g/km)...\n",
      "\n",
      "ðŸ—ºï¸ Generando GeoJSONs Maestros por Contaminante...\n",
      "   ðŸ’¾ Guardando Mapa_TOTAL_Flota_CO2_Equiv.geojson (Calles activas: 1,533,044)...\n",
      "\n",
      "ðŸš€ Â¡Ã‰XITO ROTUNDO! Proceso completado. Revisa tu carpeta: \n",
      "D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\\Output Files\\GeoJSON_Finales_Maestros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import os\n",
    "import gc \n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACIÃ“N DE UNIFICACIÃ“N\n",
    "# ==============================================================================\n",
    "BASE_DIR = Path(r\"D:\\NB3_VISUM plus MOVES merge\\Integration Files V1\")\n",
    "GEOJSON_DIR = BASE_DIR / \"Output Files\" / \"GeoJSON_Finales_Maestros\"\n",
    "GEOJSON_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# POLLUTANTS = ['CO2', 'CO2_Equiv', 'CO', 'NOx', 'Total_PM25', 'Total_PM10', 'TotalHC', 'CH4', 'N2O']\n",
    "POLLUTANTS = ['CO2_Equiv']\n",
    "\n",
    "def optimizar_memoria(df):\n",
    "    df['LinkID'] = df['LinkID'].astype('int64') \n",
    "    df['Hour'] = pd.to_numeric(df['Hour'], downcast='integer')\n",
    "    for col in POLLUTANTS:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CARGA Y COMBINACIÃ“N ESPACIAL (WKT JOIN)\n",
    "# ==============================================================================\n",
    "print(\"ðŸŒ INICIANDO LA GRAN UNIFICACIÃ“N (Auto + Carga + TP)...\")\n",
    "\n",
    "# --- A. Procesar Auto ---\n",
    "print(\"   -> Integrando Auto y Carga...\")\n",
    "df_geom_auto = pd.read_parquet(BASE_DIR / \"Links_Geometry_Master.parquet\")\n",
    "# ðŸŒŸ CORRECCIÃ“N AQUÃ: Forzar LinkID a entero en la geometrÃ­a\n",
    "df_geom_auto['LinkID'] = df_geom_auto['LinkID'].astype('int64')\n",
    "\n",
    "df_dd_auto = optimizar_memoria(pd.read_parquet(BASE_DIR / \"Link_Emissions_DistBased_Day_5.parquet\"))\n",
    "df_di_auto = optimizar_memoria(pd.read_parquet(BASE_DIR / \"Link_Emissions_VolBased_Day_5.parquet\"))\n",
    "\n",
    "df_auto = pd.concat([df_dd_auto, df_di_auto], ignore_index=True)\n",
    "df_auto = df_auto.groupby(['LinkID', 'Hour'], as_index=False)[POLLUTANTS].sum()\n",
    "# Le pegamos el WKTPOLYWGS84 para poder sumar con los buses\n",
    "df_auto = pd.merge(df_auto, df_geom_auto[['LinkID', 'WKTPOLYWGS84']], on='LinkID', how='inner')\n",
    "df_auto.drop(columns=['LinkID'], inplace=True)\n",
    "del df_geom_auto, df_dd_auto, df_di_auto; gc.collect()\n",
    "\n",
    "# --- B. Procesar TP ---\n",
    "print(\"   -> Integrando Transporte PÃºblico...\")\n",
    "df_geom_tp = pd.read_parquet(BASE_DIR / \"Transit_Geometry_Master.parquet\")\n",
    "# ðŸŒŸ CORRECCIÃ“N AQUÃ: Forzar LinkID a entero en la geometrÃ­a\n",
    "df_geom_tp['LinkID'] = df_geom_tp['LinkID'].astype('int64')\n",
    "\n",
    "df_dd_tp = optimizar_memoria(pd.read_parquet(BASE_DIR / \"Transit_Emissions_DistBased_Day_5.parquet\"))\n",
    "df_di_tp = optimizar_memoria(pd.read_parquet(BASE_DIR / \"Transit_Emissions_VolBased_Day_5.parquet\"))\n",
    "\n",
    "df_tp = pd.concat([df_dd_tp, df_di_tp], ignore_index=True)\n",
    "df_tp = df_tp.groupby(['LinkID', 'Hour'], as_index=False)[POLLUTANTS].sum()\n",
    "df_tp = pd.merge(df_tp, df_geom_tp[['LinkID', 'WKTPOLYWGS84']], on='LinkID', how='inner')\n",
    "df_tp.drop(columns=['LinkID'], inplace=True)\n",
    "del df_geom_tp, df_dd_tp, df_di_tp; gc.collect()\n",
    "\n",
    "# --- C. FusiÃ³n Total y AgrupaciÃ³n Espacial ---\n",
    "print(\"   -> Fusionando Flotas sobre la misma geometrÃ­a fÃ­sica...\")\n",
    "df_master = pd.concat([df_auto, df_tp], ignore_index=True)\n",
    "# Agrupamos por WKT y Hora. Si un bus y un auto pasan por la misma calle, se suman.\n",
    "df_master = df_master.groupby(['WKTPOLYWGS84', 'Hour'], as_index=False)[POLLUTANTS].sum()\n",
    "del df_auto, df_tp; gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CREACIÃ“N DE GEOMETRÃA Y CÃLCULO PERFECTO DE LONGITUD\n",
    "# ==============================================================================\n",
    "print(\"   -> Transformando a GeoDataFrame...\")\n",
    "df_master['geometry'] = df_master['WKTPOLYWGS84'].astype(str).apply(wkt.loads)\n",
    "df_master.drop(columns=['WKTPOLYWGS84'], inplace=True)\n",
    "\n",
    "gdf_final = gpd.GeoDataFrame(df_master, geometry='geometry')\n",
    "gdf_final.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "print(\"   -> Recalculando longitud fÃ­sica exacta (UTM Monterrey)...\")\n",
    "# El cÃ¡lculo de longitud real en UTM 14N (EPSG:32614) que arregla la visualizaciÃ³n de densidad\n",
    "gdf_final['L_km_real'] = gdf_final.to_crs(epsg=32614).geometry.length / 1000.0\n",
    "# Imponemos un mÃ­nimo de 1 metro para evitar divisiones por cero o atÃ­picos (incluso si Visum fallÃ³)\n",
    "gdf_final['L_km_real'] = gdf_final['L_km_real'].clip(lower=0.001)\n",
    "\n",
    "print(\"   -> Calculando densidad (g/km)...\")\n",
    "for pol in POLLUTANTS:\n",
    "    gdf_final[f'Densidad_{pol}_g_km'] = gdf_final[pol] / gdf_final['L_km_real']\n",
    "    gdf_final[f'Densidad_{pol}_g_km'] = gdf_final[f'Densidad_{pol}_g_km'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXPORTACIÃ“N\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸ—ºï¸ Generando GeoJSONs Maestros por Contaminante...\")\n",
    "for pol in POLLUTANTS:\n",
    "    gdf_export = gdf_final[gdf_final[pol] > 0].copy()\n",
    "    \n",
    "    if not gdf_export.empty:\n",
    "        export_cols = ['Hour', 'L_km_real', pol, f'Densidad_{pol}_g_km', 'geometry']\n",
    "        filename = GEOJSON_DIR / f\"Mapa_TOTAL_Flota_{pol}.geojson\"\n",
    "        print(f\"   ðŸ’¾ Guardando {filename.name} (Calles activas: {len(gdf_export):,})...\")\n",
    "        gdf_export[export_cols].rename(columns={pol: f'Total_{pol}_g'}).to_file(filename, driver='GeoJSON')\n",
    "\n",
    "print(f\"\\nðŸš€ Â¡Ã‰XITO ROTUNDO! Proceso completado. Revisa tu carpeta: \\n{GEOJSON_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
