{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbeb9d1",
   "metadata": {},
   "source": [
    "## Generation of UIDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8072ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš— Iniciando procesamiento de Auto y Carga...\n",
      "   âš¡ Aplicando optimizaciÃ³n vectorial de memoria...\n",
      "\n",
      "ðŸšŒ Iniciando procesamiento de Transporte PÃºblico...\n",
      "   âš ï¸ SALTANDO EM: No se encontrÃ³ .parquet en D:\\VISUM\\Archivos de integraciÃ³n MOVES_VISUM\\InputData\\20251002\\Transit\\Timeprofileitems\\scenario=TransitEM\n",
      "   -> Procesando [AP]: Timeprofileitems.parquet\n",
      "   -> Procesando [LM]: Timeprofileitems.parquet\n",
      "   -> Procesando [MP]: Timeprofileitems.parquet\n",
      "   -> Procesando [EA]: Timeprofileitems.parquet\n",
      "   -> Procesando [PP]: Timeprofileitems.parquet\n",
      "   -> Procesando [EV]: Timeprofileitems.parquet\n",
      "\n",
      "ðŸ”— Unificando y exportando tablas...\n",
      "âœ… Proceso completo. Todas las tablas unificadas se guardaron en:\n",
      "C:\\Users\\Eydan\\OneDrive\\Escritorio\\ITESM\\MAITEC Lab\\VISUM_MOVES Integration\\NBs for MOVES VISUM\\NB2_MOVES UIDB from VISUM\\NB2_OutputData\\MOVES_UIDB_Tables_VISUM\\Complete Tables\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURACIÃ“N GLOBAL Y RUTAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Ruta Ãšnica de Salida\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\Eydan\\OneDrive\\Escritorio\\ITESM\\MAITEC Lab\\VISUM_MOVES Integration\\NBs for MOVES VISUM\\NB2_MOVES UIDB from VISUM\\NB2_OutputData\\MOVES_UIDB_Tables_VISUM\\Complete Tables\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ParÃ¡metros Globales MOVES\n",
    "YEAR_ID = 2024\n",
    "DAY_IDS = [5, 2]\n",
    "MILES_PER_KM = 0.621371\n",
    "MOVES_SPEED_BINS = [0, 2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5, 52.5, 57.5, 62.5, 67.5, 72.5, float('inf')]\n",
    "\n",
    "HOURS_MAPPING = {\n",
    "    \"EM\": [22, 23, 0, 1, 2, 3, 4, 5], \"AP\": [6, 7, 8, 9], \"LM\": [10, 11],\n",
    "    \"MP\": [12], \"EA\": [13, 14, 15, 16], \"PP\": [17, 18], \"EV\": [19, 20, 21]\n",
    "}\n",
    "\n",
    "ALL_ROAD_TYPES = [1, 2, 3, 4, 5]\n",
    "ALL_HOURS = range(1, 25)\n",
    "ALL_BINS = range(1, 17)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURACIÃ“N AUTO Y CARGA\n",
    "# ==============================================================================\n",
    "\n",
    "INPUT_VISUM_DIR_AUTO = Path(r\"C:\\Users\\Eydan\\OneDrive\\Escritorio\\ITESM\\MAITEC Lab\\VISUM_MOVES Integration\\NBs for MOVES VISUM\\NB2_MOVES UIDB from VISUM\\NB2_IntputData\\Links Auto\")\n",
    "\n",
    "FILES_MAPPING_AUTO = {\n",
    "    \"EM\": \"scenario=AutoEM\", \"AP\": \"scenario=AutoAP\", \"LM\": \"scenario=AutoLM\",\n",
    "    \"MP\": \"scenario=AutoMP\", \"EA\": \"scenario=AutoEA\", \"PP\": \"scenario=AutoPP\", \"EV\": \"scenario=AutoEV\"\n",
    "}\n",
    "\n",
    "VOLUME_FACTORS = {\n",
    "    \"EM\": 1.0, \"AP\": 0.290781, \"LM\": 0.503228, \"MP\": 1.0, \"EA\": 0.250912, \"PP\": 0.512564, \"EV\": 0.406226 \n",
    "}\n",
    "\n",
    "AUTO_LIGHT_MAPPING = {\n",
    "    11: 0.146429251, 21: 0.320888794, 22: 0.104594992,\n",
    "    31: 0.190505261, 32: 0.220278756, 43: 0.002389634, 44: 0.014913313\n",
    "}\n",
    "\n",
    "KM_DAY_NL_REF = {\n",
    "    11: 77.77, 21: 32.98, 22: 200.00, 31: 39.35, 32: 39.35,\n",
    "    41: 107.80, 42: 142.68, 43: 107.80, 44: 107.80, \n",
    "    52: 78.23, 53: 200.00, 61: 298.70, 62: 300.00\n",
    "}\n",
    "\n",
    "TSYS_CODES = {\"AUTO_C\": \"VOL_AUTO_C\", \"CA1_C\": \"VOL_CA1_C\", \"CA2_C\": \"VOL_CA2_C\", \"CA_BD\": \"VOL_CA_BD\", \"CU_C\": \"VOL_CU_C\"}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CONFIGURACIÃ“N TRANSPORTE PÃšBLICO (TP)\n",
    "# ==============================================================================\n",
    "\n",
    "ITEMS_BASE_DIR_TP = Path(r\"D:\\VISUM\\Archivos de integraciÃ³n MOVES_VISUM\\InputData\\20251002\\Transit\\Timeprofileitems\")\n",
    "FILE_PROFILES_TP = Path(r\"C:\\Users\\Eydan\\OneDrive\\Escritorio\\ITESM\\MAITEC Lab\\VISUM_MOVES Integration\\NBs for MOVES VISUM\\NB6_UIDB for TP\\NB6_Input Data\\extract\\Timeprofiles\\scenario=TransitAP\\Timeprofiles.parquet\")\n",
    "\n",
    "HEADWAY_IS_SECONDS = True \n",
    "TRANSIT_BUS_ID = 42\n",
    "ROAD_TYPE_SPLIT_TP = {4: 0.8336, 5: 0.1664}\n",
    "ACTIVE_ROAD_TYPES_TP = [4, 5] \n",
    "\n",
    "FOLDER_MAPPING_TP = {\n",
    "    \"EM\": \"scenario=TransitEM\", \"AP\": \"scenario=TransitAP\", \"LM\": \"scenario=TransitLM\",\n",
    "    \"MP\": \"scenario=TransitMP\", \"EA\": \"scenario=TransitEA\", \"PP\": \"scenario=TransitPP\", \"EV\": \"scenario=TransitEV\"\n",
    "}\n",
    "\n",
    "HEADWAY_COLS = {\n",
    "    \"EM\": \"HEADWAYEM\", \"AP\": \"HEADWAYAP\", \"LM\": \"HEADWAYLM\",\n",
    "    \"MP\": \"HEADWAYMP\", \"EA\": \"HEADWAYEA\", \"PP\": \"HEADWAYPP\", \"EV\": \"HEADWAYEV\"\n",
    "}\n",
    "\n",
    "def clean_visum_string(val):\n",
    "    if pd.isna(val) or val == \"\": return 0.0\n",
    "    if isinstance(val, (int, float)): return float(val)\n",
    "    val_str = str(val).lower()\n",
    "    clean_str = re.sub(r'[^\\d\\.]', '', val_str)\n",
    "    try: return float(clean_str)\n",
    "    except ValueError: return 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PROCESAMIENTO DE AUTO Y CARGA (SÃšPER VECTORIZADO)\n",
    "# ==============================================================================\n",
    "print(\"ðŸš— Iniciando procesamiento de Auto y Carga...\")\n",
    "\n",
    "all_dfs_auto = []\n",
    "available_files_auto = list(INPUT_VISUM_DIR_AUTO.glob(\"*.parquet\"))\n",
    "\n",
    "for periodo, file_tag in FILES_MAPPING_AUTO.items():\n",
    "    match = next((f for f in available_files_auto if file_tag in f.name), None)\n",
    "    if match:\n",
    "        df = pd.read_parquet(match)\n",
    "        len_col = next((c for c in df.columns if c.upper() == 'LENGTH'), None)\n",
    "        df['LENGTH_CLEAN'] = df[len_col].apply(clean_visum_string) if len_col else 0.0\n",
    "        if 'TYPENO' not in df.columns: df['TYPENO'] = 0\n",
    "\n",
    "        factor = VOLUME_FACTORS.get(periodo, 1.0)\n",
    "        for tsys_code, std_name in TSYS_CODES.items():\n",
    "            col_vol_found = next((c for c in df.columns if \"VOL\" in c.upper() and tsys_code in c.upper()), None)\n",
    "            col_spd_found = next((c for c in df.columns if (\"VCUR\" in c.upper() or \"SPD\" in c.upper()) and tsys_code in c.upper()), None)\n",
    "            \n",
    "            if col_vol_found:\n",
    "                cleaned_vals = df[col_vol_found].apply(clean_visum_string)\n",
    "                df[std_name] = cleaned_vals / factor if factor not in [0, 1.0] else cleaned_vals\n",
    "            else:\n",
    "                df[std_name] = 0.0\n",
    "            \n",
    "            df[f\"SPD_{tsys_code}\"] = df[col_spd_found].apply(clean_visum_string) if col_spd_found else 0.0\n",
    "\n",
    "        df['Periodo'] = periodo\n",
    "        cols_to_keep = ['LENGTH_CLEAN', 'TYPENO', 'Periodo'] + list(TSYS_CODES.values()) + [f\"SPD_{k}\" for k in TSYS_CODES.keys()]\n",
    "        df_final = df[[c for c in cols_to_keep if c in df.columns]].copy()\n",
    "        df_final.rename(columns={'LENGTH_CLEAN': 'LENGTH'}, inplace=True)\n",
    "        all_dfs_auto.append(df_final)\n",
    "\n",
    "if not all_dfs_auto: raise ValueError(\"No se pudieron cargar datos de Auto.\")\n",
    "df_visum_master = pd.concat(all_dfs_auto, ignore_index=True)\n",
    "df_visum_master['LinkID_idx'] = df_visum_master.index # ID Ãºnico temporal para cruzar columnas\n",
    "\n",
    "print(\"   âš¡ Aplicando optimizaciÃ³n vectorial de memoria...\")\n",
    "# A. Extraer y filtrar volÃºmenes en formato largo instantÃ¡neamente\n",
    "vol_cols = list(TSYS_CODES.values())\n",
    "df_vol = df_visum_master.melt(id_vars=['LinkID_idx', 'Periodo', 'LENGTH', 'TYPENO'],\n",
    "                              value_vars=vol_cols, var_name='VOL_COL', value_name='Volume')\n",
    "df_vol = df_vol[df_vol['Volume'] > 0].copy() # ðŸ—‘ï¸ El secreto de la memoria: descartar ceros inmediatamente\n",
    "\n",
    "# B. Extraer velocidades\n",
    "spd_cols = [f\"SPD_{k}\" for k in TSYS_CODES.keys()]\n",
    "df_spd = df_visum_master.melt(id_vars=['LinkID_idx'],\n",
    "                              value_vars=spd_cols, var_name='SPD_COL', value_name='Speed_kph')\n",
    "\n",
    "# C. Cruzar Volumen con su Velocidad correspondiente\n",
    "df_vol['SPD_COL'] = df_vol['VOL_COL'].str.replace('VOL_', 'SPD_')\n",
    "df_vol = df_vol.merge(df_spd, on=['LinkID_idx', 'SPD_COL'], how='inner')\n",
    "\n",
    "# Liberar RAM\n",
    "del df_visum_master, df_spd\n",
    "gc.collect()\n",
    "\n",
    "# D. Expandir los tipos de vehÃ­culos segÃºn el mapeo\n",
    "map_rows = []\n",
    "total_auto = sum(AUTO_LIGHT_MAPPING.values())\n",
    "for k, v in AUTO_LIGHT_MAPPING.items():\n",
    "    map_rows.append({'VOL_COL': 'VOL_AUTO_C', 'sourceTypeID': k, 'share': v / total_auto})\n",
    "map_rows.extend([\n",
    "    {'VOL_COL': 'VOL_CA1_C', 'sourceTypeID': 61, 'share': 1.0},\n",
    "    {'VOL_COL': 'VOL_CA2_C', 'sourceTypeID': 61, 'share': 1.0},\n",
    "    {'VOL_COL': 'VOL_CA_BD', 'sourceTypeID': 61, 'share': 1.0},\n",
    "    {'VOL_COL': 'VOL_CU_C',  'sourceTypeID': 52, 'share': 1.0}\n",
    "])\n",
    "df_map = pd.DataFrame(map_rows)\n",
    "\n",
    "df_vol = df_vol.merge(df_map, on='VOL_COL', how='inner')\n",
    "\n",
    "# CÃ¡lculos fÃ­sicos base\n",
    "df_vol['VKT'] = df_vol['Volume'] * df_vol['LENGTH'] * df_vol['share']\n",
    "df_vol['VMT'] = df_vol['VKT'] * MILES_PER_KM\n",
    "df_vol['roadTypeID'] = np.where(df_vol['TYPENO'].isin([0, 1, 2, 10, 20]), 5, 4).astype(np.int8)\n",
    "\n",
    "# --- AUTO: sourceTypePopulation ---\n",
    "vkt_sum = df_vol.groupby('sourceTypeID')['VKT'].sum().reset_index()\n",
    "df_ref = pd.DataFrame(list(KM_DAY_NL_REF.items()), columns=['sourceTypeID', 'KM_Day_NL'])\n",
    "df_fleet_auto = pd.merge(vkt_sum, df_ref, on='sourceTypeID', how='left')\n",
    "df_fleet_auto['sourceTypePopulation'] = (df_fleet_auto['VKT'] / df_fleet_auto['KM_Day_NL']).fillna(0).round(0).astype(int)\n",
    "df_fleet_auto.insert(0, \"yearID\", YEAR_ID)\n",
    "fleet_auto = df_fleet_auto[['yearID', 'sourceTypeID', 'sourceTypePopulation']]\n",
    "\n",
    "# --- AUTO: VMT ---\n",
    "vmt_base = df_vol.groupby('sourceTypeID')['VMT'].sum().reset_index()\n",
    "grid_dates = list(itertools.product([YEAR_ID], range(1, 13), DAY_IDS))\n",
    "df_dates = pd.DataFrame(grid_dates, columns=['yearID', 'monthID', 'dayID'])\n",
    "vmt_auto = vmt_base.merge(df_dates, how='cross')[[\"sourceTypeID\", \"yearID\", \"monthID\", \"dayID\", \"VMT\"]]\n",
    "vmt_auto['VMT'] = vmt_auto['VMT'].round(4)\n",
    "\n",
    "# --- AUTO: roadTypeVMTFraction ---\n",
    "vmt_by_road = df_vol.groupby(['sourceTypeID', 'roadTypeID'])['VMT'].sum().reset_index()\n",
    "vmt_by_road['roadTypeVMTFraction'] = vmt_by_road['VMT'] / vmt_by_road.groupby('sourceTypeID')['VMT'].transform('sum')\n",
    "\n",
    "unique_sources = vmt_by_road['sourceTypeID'].unique()\n",
    "grid = pd.DataFrame(list(itertools.product(unique_sources, ALL_ROAD_TYPES)), columns=['sourceTypeID', 'roadTypeID'])\n",
    "road_auto = pd.merge(grid, vmt_by_road[['sourceTypeID', 'roadTypeID', 'roadTypeVMTFraction']], on=['sourceTypeID', 'roadTypeID'], how='left').fillna(0)\n",
    "\n",
    "# --- AUTO: ConfiguraciÃ³n Horaria y Velocidad ---\n",
    "hours_df = pd.DataFrame([(p, h) for p, hours in HOURS_MAPPING.items() for h in hours], columns=['Periodo', 'Hour'])\n",
    "period_lengths = {p: len(h) for p, h in HOURS_MAPPING.items()}\n",
    "df_vol['Period_Hours'] = df_vol['Periodo'].map(period_lengths)\n",
    "\n",
    "df_vol['Speed_mph'] = df_vol['Speed_kph'].clip(lower=1.6) * MILES_PER_KM\n",
    "df_vol['BIN_ID'] = np.digitize(df_vol['Speed_mph'], MOVES_SPEED_BINS).clip(max=16)\n",
    "df_vol['Time_per_hour'] = (df_vol['VMT'] / df_vol['Speed_mph']) / df_vol['Period_Hours']\n",
    "df_vol['VMT_per_hour'] = df_vol['VMT'] / df_vol['Period_Hours']\n",
    "\n",
    "# --- AUTO: hourVMTFraction ---\n",
    "df_hour_merge = df_vol[['sourceTypeID', 'roadTypeID', 'Periodo', 'VMT_per_hour']].merge(hours_df, on='Periodo')\n",
    "df_hour_merge['hourID'] = df_hour_merge['Hour'] + 1\n",
    "vmt_hour = df_hour_merge.groupby(['sourceTypeID', 'roadTypeID', 'hourID'])['VMT_per_hour'].sum().reset_index()\n",
    "vmt_hour['hourVMTFraction'] = vmt_hour['VMT_per_hour'] / vmt_hour.groupby(['sourceTypeID', 'roadTypeID'])['VMT_per_hour'].transform('sum')\n",
    "\n",
    "final_hours = []\n",
    "for day in DAY_IDS:\n",
    "    d = vmt_hour.copy()\n",
    "    d['dayID'] = day\n",
    "    final_hours.append(d)\n",
    "hour_auto = pd.concat(final_hours)[['sourceTypeID', 'roadTypeID', 'dayID', 'hourID', 'hourVMTFraction']]\n",
    "del df_hour_merge; gc.collect()\n",
    "\n",
    "# --- AUTO: avgSpeedFraction ---\n",
    "df_speed_merge = df_vol[['sourceTypeID', 'roadTypeID', 'Periodo', 'BIN_ID', 'Time_per_hour']].merge(hours_df, on='Periodo')\n",
    "df_speed_merge['hourID'] = df_speed_merge['Hour'] + 1\n",
    "grp_speed = df_speed_merge.groupby(['sourceTypeID', 'roadTypeID', 'hourID', 'BIN_ID'])['Time_per_hour'].sum().reset_index()\n",
    "grp_speed.rename(columns={'BIN_ID': 'avgSpeedBinID'}, inplace=True)\n",
    "\n",
    "# Llenar huecos de speed bins con ceros\n",
    "unique_base = grp_speed[['sourceTypeID', 'roadTypeID', 'hourID']].drop_duplicates()\n",
    "grid_full = unique_base.merge(pd.DataFrame({'avgSpeedBinID': ALL_BINS}), how='cross')\n",
    "merged_speed = pd.merge(grid_full, grp_speed, on=['sourceTypeID', 'roadTypeID', 'hourID', 'avgSpeedBinID'], how='left').fillna(0)\n",
    "merged_speed['avgSpeedFraction'] = merged_speed['Time_per_hour'] / merged_speed.groupby(['sourceTypeID', 'roadTypeID', 'hourID'])['Time_per_hour'].transform('sum')\n",
    "merged_speed['avgSpeedFraction'] = merged_speed['avgSpeedFraction'].fillna(0)\n",
    "\n",
    "final_rows = []\n",
    "for day in DAY_IDS:\n",
    "    d = merged_speed.copy()\n",
    "    d['hourDayID'] = (d['hourID'].astype(str) + str(day)).astype(np.int64)\n",
    "    final_rows.append(d)\n",
    "speed_auto = pd.concat(final_rows)[['sourceTypeID', 'roadTypeID', 'hourDayID', 'avgSpeedBinID', 'avgSpeedFraction']]\n",
    "\n",
    "del df_vol, df_speed_merge; gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. PROCESAMIENTO TRANSPORTE PÃšBLICO (TP)\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸšŒ Iniciando procesamiento de Transporte PÃºblico...\")\n",
    "\n",
    "try: df_profiles = pd.read_parquet(FILE_PROFILES_TP)\n",
    "except: df_profiles = pd.read_csv(FILE_PROFILES_TP)\n",
    "df_profiles.columns = [c.strip().upper() for c in df_profiles.columns]\n",
    "\n",
    "all_period_data_tp = []\n",
    "for period_code, folder_name in FOLDER_MAPPING_TP.items():\n",
    "    current_path = ITEMS_BASE_DIR_TP / folder_name\n",
    "    found_file = list(current_path.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not found_file: \n",
    "        print(f\"   âš ï¸ SALTANDO {period_code}: No se encontrÃ³ .parquet en {current_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   -> Procesando [{period_code}]: {found_file[0].name}\")\n",
    "    df_items = pd.read_parquet(found_file[0])\n",
    "    df_items.columns = [c.strip().upper() for c in df_items.columns]\n",
    "    \n",
    "    col_len = 'POSTLENGTH' if 'POSTLENGTH' in df_items.columns else 'LENGTH'\n",
    "    df_items['LENGTH_KM'] = df_items[col_len].apply(clean_visum_string)\n",
    "    df_items['SPEED_KPH'] = df_items['VEL'].apply(clean_visum_string) if 'VEL' in df_items.columns else 0.0\n",
    "        \n",
    "    merge_keys = [c for c in ['LINENAME', 'LINEROUTENAME', 'DIRECTIONCODE', 'LINEROUTEID'] if c in df_items.columns and c in df_profiles.columns]\n",
    "    df_merged = pd.merge(df_items, df_profiles, on=merge_keys, how='left')\n",
    "    \n",
    "    hw_col = HEADWAY_COLS.get(period_code)\n",
    "    if hw_col and hw_col in df_merged.columns:\n",
    "        headway_vals = df_merged[hw_col].apply(clean_visum_string) * (1.0 if HEADWAY_IS_SECONDS else 60.0)\n",
    "        freq_per_hour = np.where(headway_vals > 0, 3600 / headway_vals, 0)\n",
    "        df_merged['VOL_PERIOD'] = freq_per_hour * len(HOURS_MAPPING[period_code])\n",
    "    else:\n",
    "        df_merged['VOL_PERIOD'] = 0.0\n",
    "\n",
    "    df_merged['PERIOD_CODE'] = period_code\n",
    "    all_period_data_tp.append(df_merged[['PERIOD_CODE', 'LENGTH_KM', 'SPEED_KPH', 'VOL_PERIOD']].copy())\n",
    "\n",
    "if not all_period_data_tp: \n",
    "    raise ValueError(f\"âŒ No se generaron datos de TP. Ninguna de las carpetas en {ITEMS_BASE_DIR_TP} tenÃ­a archivos .parquet\")\n",
    "\n",
    "df_master_bus = pd.concat(all_period_data_tp, ignore_index=True)\n",
    "\n",
    "# --- TP: sourceTypePopulation & VMT ---\n",
    "total_vmt_miles = (df_master_bus['VOL_PERIOD'] * df_master_bus['LENGTH_KM'] * MILES_PER_KM).sum()\n",
    "pop_bus = int(round((total_vmt_miles / MILES_PER_KM) / KM_DAY_NL_REF[42])) if KM_DAY_NL_REF[42] > 0 else 0\n",
    "\n",
    "fleet_tp = pd.DataFrame([{'yearID': YEAR_ID, 'sourceTypeID': TRANSIT_BUS_ID, 'sourceTypePopulation': pop_bus}])\n",
    "\n",
    "grid_dates_tp = list(itertools.product([YEAR_ID], range(1, 13), DAY_IDS))\n",
    "df_vmt_tp = pd.DataFrame(grid_dates_tp, columns=['yearID', 'monthID', 'dayID'])\n",
    "df_vmt_tp['sourceTypeID'] = TRANSIT_BUS_ID\n",
    "df_vmt_tp['VMT'] = total_vmt_miles\n",
    "vmt_tp = df_vmt_tp[[\"sourceTypeID\", \"yearID\", \"monthID\", \"dayID\", \"VMT\"]].round(4)\n",
    "\n",
    "# --- TP: roadTypeVMTFraction ---\n",
    "grid_rt = pd.DataFrame(list(itertools.product([TRANSIT_BUS_ID], ALL_ROAD_TYPES)), columns=['sourceTypeID', 'roadTypeID'])\n",
    "grid_rt['roadTypeVMTFraction'] = grid_rt['roadTypeID'].map(ROAD_TYPE_SPLIT_TP).fillna(0.0)\n",
    "total_frac = grid_rt['roadTypeVMTFraction'].sum()\n",
    "if not np.isclose(total_frac, 1.0): grid_rt['roadTypeVMTFraction'] = grid_rt['roadTypeVMTFraction'] / total_frac\n",
    "road_tp = grid_rt\n",
    "\n",
    "# --- TP: avgSpeedFraction ---\n",
    "speed_base_records = []\n",
    "df_active = df_master_bus[df_master_bus['VOL_PERIOD'] > 0].copy()\n",
    "if not df_active.empty:\n",
    "    df_active['SPEED_MPH'] = df_active['SPEED_KPH'] * MILES_PER_KM\n",
    "    df_active['SPEED_MPH'] = df_active['SPEED_MPH'].replace(0, 1.0)\n",
    "    df_active['DIST_MILES'] = df_active['LENGTH_KM'] * MILES_PER_KM\n",
    "    df_active['TOTAL_TIME_H'] = df_active['VOL_PERIOD'] * (df_active['DIST_MILES'] / df_active['SPEED_MPH'])\n",
    "    df_active['BIN_ID'] = np.digitize(df_active['SPEED_MPH'], MOVES_SPEED_BINS)\n",
    "    df_active.loc[df_active['BIN_ID'] > 16, 'BIN_ID'] = 16\n",
    "    grouped_speed = df_active.groupby(['PERIOD_CODE', 'BIN_ID'])['TOTAL_TIME_H'].sum()\n",
    "\n",
    "    for (period, bin_id), total_time in grouped_speed.items():\n",
    "        hours_list = HOURS_MAPPING.get(period, [])\n",
    "        if not hours_list: continue\n",
    "        time_per_hour = total_time / len(hours_list)\n",
    "        for h in hours_list:\n",
    "            speed_base_records.append({'hourID': h + 1, 'avgSpeedBinID': bin_id, 'time_weight': time_per_hour})\n",
    "\n",
    "df_base = pd.DataFrame(speed_base_records)\n",
    "if not df_base.empty:\n",
    "    df_base['avgSpeedFraction'] = df_base['time_weight'] / df_base.groupby('hourID')['time_weight'].transform('sum')\n",
    "else:\n",
    "    df_base = pd.DataFrame(columns=['hourID', 'avgSpeedBinID', 'avgSpeedFraction'])\n",
    "\n",
    "df_grid = pd.DataFrame(list(itertools.product([TRANSIT_BUS_ID], ALL_ROAD_TYPES, ALL_HOURS, ALL_BINS)), columns=['sourceTypeID', 'roadTypeID', 'hourID', 'avgSpeedBinID'])\n",
    "df_merged = pd.merge(df_grid, df_base[['hourID', 'avgSpeedBinID', 'avgSpeedFraction']], on=['hourID', 'avgSpeedBinID'], how='left')\n",
    "df_merged['avgSpeedFraction'] = df_merged['avgSpeedFraction'].fillna(0)\n",
    "\n",
    "df_merged.loc[~df_merged['roadTypeID'].isin(ACTIVE_ROAD_TYPES_TP), 'avgSpeedFraction'] = 0\n",
    "sums = df_merged.groupby(['sourceTypeID', 'roadTypeID', 'hourID'])['avgSpeedFraction'].transform('sum')\n",
    "df_merged.loc[(sums == 0) & (df_merged['avgSpeedBinID'] == 1), 'avgSpeedFraction'] = 1.0\n",
    "\n",
    "final_rows = []\n",
    "for day in DAY_IDS:\n",
    "    d = df_merged.copy()\n",
    "    d['hourDayID'] = (d['hourID'].astype(str) + str(day)).astype(np.int64)\n",
    "    final_rows.append(d)\n",
    "speed_tp = pd.concat(final_rows)[['sourceTypeID', 'roadTypeID', 'hourDayID', 'avgSpeedBinID', 'avgSpeedFraction']]\n",
    "\n",
    "# --- TP: hourVMTFraction ---\n",
    "df_master_bus['VMT_SEGMENT'] = df_master_bus['VOL_PERIOD'] * df_master_bus['LENGTH_KM'] * MILES_PER_KM\n",
    "vmt_by_period = df_master_bus.groupby('PERIOD_CODE')['VMT_SEGMENT'].sum()\n",
    "hourly_vmt_base = {h: 0.0 for h in ALL_HOURS}\n",
    "for period, vmt_total in vmt_by_period.items():\n",
    "    hours_list = HOURS_MAPPING.get(period, [])\n",
    "    if not hours_list: continue\n",
    "    for h in hours_list: hourly_vmt_base[h + 1] = vmt_total / len(hours_list) \n",
    "\n",
    "total_daily_vmt = sum(hourly_vmt_base.values())\n",
    "hourly_profile = {h: vmt / total_daily_vmt for h, vmt in hourly_vmt_base.items()} if total_daily_vmt > 0 else {h: 1.0 / 24.0 for h in ALL_HOURS}\n",
    "\n",
    "grid_h_records = []\n",
    "for rt in ALL_ROAD_TYPES:\n",
    "    for h in ALL_HOURS:\n",
    "        grid_h_records.append({\n",
    "            'sourceTypeID': TRANSIT_BUS_ID, 'roadTypeID': rt, 'hourID': h,\n",
    "            'hourVMTFraction': hourly_profile.get(h, 0.0) if rt in ACTIVE_ROAD_TYPES_TP else (1.0 / 24.0)\n",
    "        })\n",
    "\n",
    "final_h_list = []\n",
    "df_hour = pd.DataFrame(grid_h_records)\n",
    "for day in DAY_IDS:\n",
    "    d = df_hour.copy()\n",
    "    d['dayID'] = day\n",
    "    final_h_list.append(d)\n",
    "hour_tp = pd.concat(final_h_list)[['sourceTypeID', 'roadTypeID', 'dayID', 'hourID', 'hourVMTFraction']]\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. UNIFICACIÃ“N Y EXPORTACIÃ“N FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸ”— Unificando y exportando tablas...\")\n",
    "\n",
    "final_fleet = pd.concat([fleet_auto, fleet_tp], ignore_index=True)\n",
    "final_vmt   = pd.concat([vmt_auto, vmt_tp], ignore_index=True)\n",
    "final_road  = pd.concat([road_auto, road_tp], ignore_index=True)\n",
    "final_speed = pd.concat([speed_auto, speed_tp], ignore_index=True)\n",
    "final_hour  = pd.concat([hour_auto, hour_tp], ignore_index=True)\n",
    "\n",
    "final_fleet.to_csv(OUTPUT_DIR / \"sourceTypePopulation.csv\", index=False)\n",
    "final_vmt.to_csv(OUTPUT_DIR / \"VMT.csv\", index=False)\n",
    "final_road.to_csv(OUTPUT_DIR / \"roadTypeVMTFraction.csv\", index=False)\n",
    "final_speed.to_csv(OUTPUT_DIR / \"avgSpeedFraction.csv\", index=False)\n",
    "final_hour.to_csv(OUTPUT_DIR / \"hourVMTFraction.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Proceso completo. Todas las tablas unificadas se guardaron en:\\n{OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
